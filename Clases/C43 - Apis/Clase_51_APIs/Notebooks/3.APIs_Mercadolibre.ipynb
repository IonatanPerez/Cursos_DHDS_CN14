{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs - Práctica Guiada III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "## Tabla de Contenidos\n",
    "\n",
    "[1- Intro](#section_intro)\n",
    "\n",
    "$\\hspace{.5cm}$[1.1- La librería requests](#section_requests)\n",
    "\n",
    "[2- Formando un DataFrame](#section_df)\n",
    "\n",
    "$\\hspace{.5cm}$[2.1- Parseando un JSON con Pandas](#section_json_normalize)\n",
    "\n",
    "$\\hspace{.5cm}$[2.2- Data Wrangling](#section_data_wrangling)\n",
    "\n",
    "[3- Paginado: cómo armar el DataFrame final](#section_paginado)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "###  1- Intro\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "El objetivo de esta práctica guiada es generar el dataset que utilizaremos en el ejercicio de la clase. Para ello, utilizaremos funcionalidades un poco más avanzadas de la librería `requests`, la cual tiene una reputación notable como una de las librerías más \"Pythonicas\" que existen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_requests\"></a> \n",
    "#### 1.1- La librería requests\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Como explicamos en la clase teórica, el protocolo HTTP nos permite emplear distintas \"acciones\" por medio de las requests HTTP. La librería requests es un paquete de Python que contiene métodos sencillos y lineales para ejecutar estas Requests (GET, POST, PUT, DELETE).\n",
    "\n",
    "Cuando una API no tiene una librería desarrollada para consumirse (o incluso cuando la tiene pero nos resulta incómoda o restrictiva), la librería requests nos permite directamente traer los resultados de ejecutar una HTTP request al endpoint que deseemos, siempre y cuando tengamos la posibilidad de acceder a él (lógicamente no es muy sencillo intentar mandar una DELETE request a la API de Mercadolibre, por ejemplo, a menos que seamos los propietarios de una determinada publicación). \n",
    "\n",
    "Para traernos datos, lo más habitual, y lo que usaremos en esta práctica, es una request tipo GET. Si esta request es exitosa, nos traerá un mensaje (hoy en día lo más común es el formato JSON) con la información de la página que pedimos ver. A continuación, veamos un ejemplo sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dYbWIrX4CSuD"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://api.mercadolibre.com/sites/MLA/search?q=nintendo+switch&condition=new&limit=50\"  # Endpoint a consultar\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta a una request es un objeto tipo `requests.models.Response`. Este tipo de objetos contiene, además del texto del mensaje de respuesta, un código de status. Podemos ver en esta imagen el significado de distintos códigos de respuesta:\n",
    "\n",
    "<img src='./img/statuscode2.jpg' align='center'/>\n",
    "\n",
    "Para una request de tipo GET, un código 200 significa que todo está bien. Veamos qué status obtenemos para nuestra consulta a Mercadolibre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "brp_2rQxCSuG",
    "outputId": "63dbb281-9fd9-423a-ce65-b214125d8458"
   },
   "outputs": [],
   "source": [
    "print(\"Status Code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora el encabezado de la respuesta. De aquí podemos obtener varios datos interesantes, como cuándo fue hecha la request y qué encoding tiene el mensaje que nos devuelve en la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "8xwaG029CSuM",
    "outputId": "59113462-6678-44ab-ee6c-6f76a5dfb97e"
   },
   "outputs": [],
   "source": [
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el atributo `response.text` de la respuesta, obtenemos el contenido del mensaje. Por ser esta una request tipo get, pueden hacer la prueba de ingresar a la URL de la request en su navegador, y observarán el json del resultado que aquí obtenemos en forma programática.\n",
    "\n",
    "El texto viene como un string, pero queremos poder tratarlo como una estructura de datos, con lo cual vamos a parsear el json con la librería `json` de Python. Esta librería viene con cualquier instalación de Python, por lo cual no requiere setup adicional. `json.loads()` convertirá la respuesta en objetos nativos de Python (diccionarios y listas), con el fin de poder armar nuestro dataset.\n",
    "\n",
    "En el caso particular de la respuesta de Mercadolibre, dentro del json encontramos una clave \"results\", en la cual encontraremos toda publicación perteneciente a la respuesta. Vamos a quedarnos con los resultados en nuestra variable `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "p0PuCIqzCSuQ",
    "outputId": "ada9a45b-2e8b-413e-9b5c-c88255dd9b6a"
   },
   "outputs": [],
   "source": [
    "data = json.loads(response.text)\n",
    "\n",
    "results=data[\"results\"]\n",
    "\n",
    "# print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_df\"></a> \n",
    "###  2- Formando un DataFrame\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Para el ejercicio de esta clase, vamos a querer usar un DataFrame de publicaciones de Mercadolibre con el fin de predecir la cantidad de unidades vendidas. Para ello, queremos formar un DataFrame de pandas y formar un CSV. Usar el constructor de DataFrame sobre nuestra variable `results` tiene varias limitaciones muy serias, que iremos resolviendo conforme avancemos en la práctica, vamos a ver algunas de ellas a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "tSbTQ6nZCSuX",
    "outputId": "9186dec8-75fc-4927-93ce-c7403503b31e"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este primer resultado podemos observar varios de los problemas de los que hablábamos anteriormente:\n",
    "* Muchas de nuestras columnas son diccionarios. ¿Cómo podemos acceder a los campos en cada uno?\n",
    "* Sólo tenemos 50 registros. ¿Qué pasa si modificamos el parámetro `limit` en nuestra request?\n",
    "* Muchas de las publicaciones que vemos no refieren al producto que buscamos, sino que nos hablan de accesorios para el mismo. ¿Es posible filtrar estas respuestas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_json_normalize\"></a> \n",
    "#### 2.1- Parseando un JSON con Pandas\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Afortunadamente, como dijimos antes, el formato JSON se ha vuelto un estándar ampliamente difundido en los respuestas de APIs y, como tal, Pandas tiene su propia implementación para leer información de JSONs y expandir los subcampos.\n",
    "\n",
    "Por todo lo susodicho, la función `json_normalize` de Pandas resuelve en forma sencilla y completa el primero de los tres problemas que teníamos anteriormente. Veamos cómo queda ese DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "tSbTQ6nZCSuX",
    "outputId": "9186dec8-75fc-4927-93ce-c7403503b31e"
   },
   "outputs": [],
   "source": [
    "df_expand=pd.json_normalize(results)\n",
    "df_expand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que nuestro nuevo DataFrame tiene muchas más columnas. Comparemos ambos índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame original:\", list(df.columns))\n",
    "print()\n",
    "print(\"DataFrame expandido:\", list(df_expand.columns))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, el DataFrame expandido trae dos problemas adicionales:\n",
    "1. Genera demasiadas columnas, ya que no todos los subcampos nos son relevantes.\n",
    "1. Muestra los niveles de expansión por medio de puntos, lo cual nos molestaría para la notación de atributos (por ejemplo `df_expand.seller.seller_reputation.real_level` nos arrojaría un error, en lugar de devolver la serie `df_expand[\"seller.seller_reputation.real_level\"]`.\n",
    "\n",
    "Estos dos problemas se resuelven sencillamente, renombrando las columnas y luego quedándonos sólo con las relevantes. Ensallaremos una solución para estos problemas, que luego reproduciremos en nuestro dataset final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_data_wrangling\"></a> \n",
    "#### 2.2- Data Wrangling\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Antes de generar un DataFrame de un tamaño importante para efectuar predicciones, queremos dejar armadas funciones para poder limpiarlo en forma rápida. Comenzaremos renombrando columnas para remover esos puntos molestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_json_cols(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [col.replace(\".\", \"__\") for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dots = rename_json_cols(df_expand)\n",
    "df_no_dots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar removeremos las columnas geográficas, con todos los subelementos correspondientes, así como los ids innecesarios, y las variables categóricas que sólo tengan un valor posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, drop_geo=True, drop_ids=True, drop_uniques=True):\n",
    "    \"\"\"\n",
    "    Función para eliminar columnas innecesarias del DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    drop_geo : bool\n",
    "        Booleano para definir si eliminamos las veriables geográficas. Default, True.\n",
    "    drop_ids: bool\n",
    "        Booleano para definir si eliminamos las veriables de IDs y similares. Default, True.\n",
    "    drop_uniques: bool\n",
    "        Booleano para definir si eliminamos las veriables con un solo valor posible. Default, True.\n",
    "\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Variables geográficas:\n",
    "    if drop_geo:\n",
    "        nogeo_mask = df.columns[\n",
    "            ~df.columns.str.contains(r\"^address__|^seller_address__|^seller__eshop\")\n",
    "        ]\n",
    "        df = df[nogeo_mask]\n",
    "    if drop_uniques:\n",
    "        series_uniques = df.apply(lambda x: len(x.value_counts()) != 1).values\n",
    "        df = df.loc[:, series_uniques]\n",
    "    if drop_ids:\n",
    "        for col in [\"permalink\", \"seller__permalink\", \"catalog_product_id\",\n",
    "                    \"thumbnail\", \"attributes\", \"original_price\", \"official_store_id\"]:\n",
    "            try:\n",
    "                df = df.drop(col, axis=1)\n",
    "            except Exception:\n",
    "                print(f\"{col} ya fue eliminada en chequeos anteriores.\")\n",
    "        series_ids = df.apply(lambda x: len(x.value_counts()) != len(df)).values\n",
    "        df = df.loc[:, series_ids]\n",
    "    # También columnas excesivamente nulas.\n",
    "    series_nulls = df.isnull().mean() < 0.6\n",
    "    df = df.loc[:, series_nulls]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pruned = drop_columns(df_no_dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pruned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto nos quedamos con un df bastante terminado, pero tenemos dos columnas que nos resultan interesantes por ser fechas: `seller__registration_date` y `stop_time`. Queremos convertir la primera en años de actividad del vendedor, y la segunda en días restantes a la publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(df):\n",
    "    \"\"\"Convierte 'stop_time' en 'days_remaining' y 'seller__registration_date' en 'years_active'.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"days_remaining\"] = (\n",
    "        pd.to_datetime(df['stop_time'], utc=True) - pd.Timestamp(datetime.utcnow(), tz=\"UTC\")\n",
    "    ).dt.days\n",
    "    df[\"years_active\"] = (\n",
    "        pd.Timestamp(datetime.utcnow(), tz=\"UTC\") -  pd.to_datetime(df['seller__registration_date'], utc=True)\n",
    "    ).dt.days // 365\n",
    "    df = df.drop([\"stop_time\", \"seller__registration_date\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = parse_dates(df_pruned)\n",
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos un DataFrame cuyo formato parece apto para predecir, ahora intentemos obtener un dataframe de mayor tamaño, y enfocado sólo en el tipo de producto que buscamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_paginado\"></a> \n",
    "###  3- Paginado: cómo armar el DataFrame final\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "En esta última sección vamos a resolver los dos problemas que nos quedan: el reducido tamaño del dataset, y los productos \"accesorios\" al que realmente estamos buscando. Para esto usaremos el dataset a escala que generamos antes, con el fin de determinar a qué categoría pertenece en la API de Mercadolibre nuestro producto buscado, y efectuaremos una request con paginado, para obtener algunos registros más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengamos primero la categoría.\n",
    "df_expand.domain_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que la categoría que corresponde al producto es también la más usual. Sin embargo, corresponde a un poco más de la mitad de los registros de la respuesta. Como tenemos un límite a cuántos datos podemos pedir, lo más inteligente es incluir ese filtro en la request misma. Utilicemos ahora la categoría (`category_id`), con el fin de obtener un código que sirva como parámetro en la API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_expand.category_id.value_counts())\n",
    "category = df_expand.category_id.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.mercadolibre.com/sites/MLA/search?\"\n",
    "params = {\n",
    "    \"q\": \"nintendo switch\".replace(\" \", \"+\"),\n",
    "    \"limit\": 50,\n",
    "    \"condition\": \"new\",\n",
    "    \"category\": category\n",
    "}\n",
    "# q=nintendo+switch&condition=new&limit=50&category=MLA438566  # Endpoint a consultar\n",
    "response = requests.get(url, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(json.loads(response.text)[\"results\"]).domain_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta parametrización logra traernos sólo los datos que buscamos, con lo que sólo nos queda resolver el problema del paginado. Vamos a armar una solución que vaya de la request al dataset finalizado, con el fin de poder generar un dataset del tema que a ustedes les interese:\n",
    "\n",
    "**NOTA IMPORTANTE:** la API tiene un límite de requests máximas permitidas, así que no ejecuten esta celda muchas veces seguidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_request(search=\"iphone 11\", limit=50, total_obs=2000, save=True, category=None, access_token=None):\n",
    "    url = \"https://api.mercadolibre.com/sites/MLA/search?\"\n",
    "    results = []\n",
    "    if category is None:\n",
    "        params = {\n",
    "            \"q\": search.replace(\" \", \"+\"),\n",
    "            \"limit\": 50,\n",
    "            \"condition\": \"new\",\n",
    "        }\n",
    "        response_ = requests.get(url, params=params)\n",
    "        cats_ = pd.DataFrame(json.loads(response_.text)[\"results\"]).category_id.value_counts()\n",
    "        category = cats_.index[0]\n",
    "        del cats_, response_\n",
    "    params[\"category\"] = category\n",
    "    params[\"limit\"] = limit\n",
    "    params[\"access_token\"] = access_token\n",
    "    for offset in range(0, total_obs, limit):  # Paginamos usando el parámetro\n",
    "        try:\n",
    "            params[\"offset\"] = offset\n",
    "            response = requests.get(url, params=params)\n",
    "            results += response.json()[\"results\"]\n",
    "        except KeyError:\n",
    "            print(\n",
    "                \"Error: El máximo de datos permitidos fue superado o no puede encontrarse más publicaciones.\",\n",
    "                f\"Se obtuvieron {len(results)} publicaciones.\"\n",
    "            )\n",
    "            break\n",
    "    df = pd.json_normalize(results)\n",
    "    df = rename_json_cols(df)\n",
    "    df = drop_columns(df)\n",
    "    df = parse_dates(df)\n",
    "    if save:\n",
    "        df.to_csv(f'../Data/{search.lower().replace(\" \", \"_\")}_meli.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA IMPORTANTE:** Crear la carpeta \"Data\" en el directorio de la clase si aún no la tienen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = full_request(total_obs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de intentar resolver un problema predictivo sobre la cantidad de ventas, veamos cómo se distribuyen los precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "final_df.price[final_df.price < final_df.price.quantile(.99)].plot.kde()\n",
    "plt.xlabel(\"Price\");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Solution_PRACTICA_GUIADA_APIs_JSON.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
